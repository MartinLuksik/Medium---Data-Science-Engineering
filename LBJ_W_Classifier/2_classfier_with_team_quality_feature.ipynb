{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic('matplotlib notebook')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "#Load Data Set with Lebron's games and statistics\n",
    "df = pd.read_csv('Teams_LBJ_Stats.csv', sep=';')\n",
    "df = df.rename(columns={'o:team': 'team'})\n",
    "\n",
    "#Load Data Sets with seasons 2003 -> 2018 and derive the desired data in order to compute the season W%\n",
    "rang = np.arange(2003,2018,1)\n",
    "team = []\n",
    "season = []\n",
    "SeasonPercentage = []\n",
    "\n",
    "for x in rang:\n",
    "    y = pd.read_csv('/{0}'.format(x) + '.csv', sep=';')\n",
    "    y['W'] = np.where(y['W'] == True, 1, 0)\n",
    "    y = y.rename(columns={'W':'SeasonPercentage'})\n",
    "    y = y.groupby('team').agg({'SeasonPercentage': 'mean' , 'season': 'first'})\n",
    "    team.append(y.index.tolist())\n",
    "    season.append(y['season'].tolist())\n",
    "    SeasonPercentage.append(y['SeasonPercentage'].tolist())\n",
    "\n",
    "# Deriving the new feature -> quality of the opponent (Season Percentage) by merging the datasets on keys 'team' and 'season' features\n",
    "team = reduce(lambda x,y: x+y,team)\n",
    "season = reduce(lambda x,y: x+y,season)\n",
    "SeasonPercentage = reduce(lambda x,y: x+y,SeasonPercentage)\n",
    "y = {'team':team, 'season': season, 'SeasonPercentage': SeasonPercentage}\n",
    "y = pd.DataFrame(y)\n",
    "df = pd.merge(df, y, on=['team', 'season'], how='left')\n",
    "df = df.drop(['team', 'season', 'Lebron James:steals', 'Lebron James:fouls', 'Lebron James:blocks', 'Lebron James:free throws attempted'], axis=1)\n",
    "\n",
    "print(len(df))\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of Multicollinearity\n",
    "df1 = df.drop(['W'], axis=1)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X = add_constant(df1)\n",
    "pd.Series([variance_inflation_factor(X.values, i)\n",
    "               for i in range(X.shape[1])],\n",
    "              index=X.columns)\n",
    "# no need for a feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Validation scores on 4 folds of train and test sets in order to prevent overfitting while controling the C parameter of Logistic Regression:\n",
    "    ### 4 splits ensure that all the data points will be tested\n",
    "        ### later, the 75:25 (train set : test set) will be used\n",
    "        \n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### Splitting the data on dependent and independet variables\n",
    "X = df1\n",
    "y = df['W']\n",
    "\n",
    "### Testing the validation scores on a range of C parameters in order to find a suitable \n",
    "param_range = np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "train_scores, test_scores = validation_curve(LogisticRegression(), X, y,\n",
    "                                            param_name='C',\n",
    "                                            param_range=param_range, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with Logistic Regression')\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 1\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Validation Curve, we can see that the data set doesn't tend to over-fit -> the complexity of the dataset is in order.\n",
    "# When the C parameter = 1, the Cross-Validation score has the best performance.\n",
    "# Let's further investigate C = [0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the results, let's further investigate the accuracy, AUC and recall.\n",
    "\n",
    "# Cross-validation with the test for the variance of C-parameter (narrowed down to three options based on the Validation Curve)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for c_param in [0.1, 1, 10]:\n",
    "    clf = LogisticRegression(C=c_param)\n",
    "\n",
    "    # accuracy is the default scoring metric\n",
    "    print('For C parameter = {0}'.format(c_param) + ':')\n",
    "    print('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=4))\n",
    "    # use AUC as scoring metric\n",
    "    print('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=4, scoring = 'roc_auc'))\n",
    "    # use recall as scoring metric\n",
    "    print('Cross-validation (recall)', cross_val_score(clf, X, y, cv=4, scoring = 'recall'))\n",
    "       # use recall as scoring metric\n",
    "    print('Cross-validation (precesion)', cross_val_score(clf, X, y, cv=4, scoring = 'precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the performance with the C = 1 is performing better than C=10.\n",
    "# Also, the C=1 model outperforms the C=0.01 when it comes to AUC and accuracy. The trade off between racall and precision is relatively similar to the previous case (without the feature: Season Percentage)\n",
    "# # Based on the same arguments introduced in the 1st model, the C=1 is a suitable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the Confusion Matrix, Accuracy, Precision, and Recall for C=1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "clf_predicted = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, clf_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, clf_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the results from Model with/without the feature \"Season Percentage\" on the same train and test sets:\n",
    "    # the new feature improved the model by 1% of accuracy, 2% of precision. 2% of recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Precision-recall curves\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ### Decision functions\n",
    "y_scores_lr = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recall and Precision trade-off is captured above. The current state of the model is displayed by the red circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves for C=0,1 and C = 1\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "for g in [0.1, 1]:\n",
    "    clf = LogisticRegression(C=g).fit(X_train, y_train)\n",
    "    y_score_svm = clf.decision_function(X_test)\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    accuracy_svm = clf.score(X_test, y_test)\n",
    "    print(\"C = {:.3f}  accuracy = {:.3f}   AUC = {:.3f}\".format(g, accuracy_svm,\n",
    "                                                                    roc_auc_svm))\n",
    "    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7,\n",
    "             label='(C = {:0.3f}, area = {:0.3f})'.format(g, roc_auc_svm))\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "plt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.title('ROC curve', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The AUC for C=1 is higher, which also supports the decision to build the model this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the choice of the C parameter was done.\n",
    "# The model complexity is not leading to over/under fitting => tested on 4 folds. \n",
    "# Let's fit the model to all the data points available in order to achieve the best performance\n",
    "  # -> the number of observation has nothing to do with over fitting -> that is problem of the model complexity\n",
    "  \n",
    "# Creating new observations from the NBA Finals Game 1,2,3,4 (previously not seen in the data set) -> and predict the W probability for these games.\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X, y)\n",
    "\n",
    "# G1:\n",
    "LBJ_stats_first_game_1 = {'Lebron James:points': [51],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:field goals attempted': [32],\n",
    "                        'Lebron James:minutes': [48],\n",
    "                        'Lebron James:three pointers attempted': [7],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:rebounds':[8],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g1 = pd.DataFrame(data=LBJ_stats_first_game_1)\n",
    "y_predict1 = clf.predict_proba(g1)\n",
    "\n",
    "#G2:\n",
    "LBJ_stats_first_game_2 = {'Lebron James:points': [29],\n",
    "                        'Lebron James:assists': [13],\n",
    "                        'Lebron James:field goals attempted': [20],\n",
    "                        'Lebron James:minutes': [44],\n",
    "                        'Lebron James:three pointers attempted': [4],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:rebounds':[9],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g2 = pd.DataFrame(data=LBJ_stats_first_game_2)\n",
    "y_predict2 = clf.predict_proba(g2)\n",
    "\n",
    "#G3:\n",
    "LBJ_stats_first_game_3 = {'Lebron James:points': [33],\n",
    "                        'Lebron James:assists': [11],\n",
    "                        'Lebron James:field goals attempted': [28],\n",
    "                        'Lebron James:minutes': [47],\n",
    "                        'Lebron James:three pointers attempted': [6],\n",
    "                        'Lebron James:turnovers': [4],\n",
    "                        'Lebron James:rebounds':[10],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g3 = pd.DataFrame(data=LBJ_stats_first_game_3)\n",
    "y_predict3 = clf.predict_proba(g3)\n",
    "\n",
    "#G4:\n",
    "LBJ_stats_first_game_4 = {'Lebron James:points': [23],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:field goals attempted': [13],\n",
    "                        'Lebron James:minutes': [41],\n",
    "                        'Lebron James:three pointers attempted': [1],\n",
    "                        'Lebron James:turnovers': [6],\n",
    "                        'Lebron James:rebounds':[7],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g4 = pd.DataFrame(data=LBJ_stats_first_game_4)\n",
    "y_predict4 = clf.predict_proba(g4)\n",
    "\n",
    "print('Probability of Cavs winning the G1: ', y_predict1)\n",
    "print('Probability of Cavs winning the G2: ', y_predict2)\n",
    "print('Probability of Cavs winning the G3: ', y_predict3)\n",
    "print('Probability of Cavs winning the G4: ', y_predict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -> Even when delivering great performance in G1, the probability was only 63%.\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lebron James would need to put up these numbers to increase the change of Cleveland beating GSW to 90%:\n",
    "LBJ_stats_first_game_X = {'Lebron James:points': [50],\n",
    "                        'Lebron James:assists': [15],\n",
    "                        'Lebron James:field goals attempted': [30],\n",
    "                        'Lebron James:minutes': [42],\n",
    "                        'Lebron James:three pointers attempted': [5],\n",
    "                        'Lebron James:turnovers': [2],\n",
    "                        'Lebron James:rebounds':[10],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "gX = pd.DataFrame(data=LBJ_stats_first_game_X)\n",
    "y_predictX = clf.predict_proba(gX)\n",
    "\n",
    "print('Probability of Cavs winning the GX: ', y_predictX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how the feature importance changed compare to the previous model:\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
    "                            random_state = 0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, list(X_train))\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))\n",
    "print(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmed: The quality of the opponent is the most important feature in this model, and it overshadows LeBron's statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Teams_LBJ_Stats.csv' does not exist",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-860eb08b56b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Load Data Set with Lebron's games and statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Teams_LBJ_Stats.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'o:team'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'team'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\googleapi\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\googleapi\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\googleapi\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\googleapi\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\googleapi\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'Teams_LBJ_Stats.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "get_ipython().magic('matplotlib notebook')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "#Load Data Set with Lebron's games and statistics\n",
    "df = pd.read_csv('Teams_LBJ_Stats.csv', sep=';')\n",
    "df = df.rename(columns={'o:team': 'team'})\n",
    "\n",
    "#Load Data Sets with seasons 2003 -> 2018 and derive the desired data in order to compute the season W%\n",
    "rang = np.arange(2003,2018,1)\n",
    "team = []\n",
    "season = []\n",
    "SeasonPercentage = []\n",
    "\n",
    "for x in rang:\n",
    "    y = pd.read_csv('/{0}'.format(x) + '.csv', sep=';')\n",
    "    y['W'] = np.where(y['W'] == True, 1, 0)\n",
    "    y = y.rename(columns={'W':'SeasonPercentage'})\n",
    "    y = y.groupby('team').agg({'SeasonPercentage': 'mean' , 'season': 'first'})\n",
    "    team.append(y.index.tolist())\n",
    "    season.append(y['season'].tolist())\n",
    "    SeasonPercentage.append(y['SeasonPercentage'].tolist())\n",
    "\n",
    "# Deriving the new feature -> quality of the opponent (Season Percentage) by merging the datasets on keys 'team' and 'season' features\n",
    "team = reduce(lambda x,y: x+y,team)\n",
    "season = reduce(lambda x,y: x+y,season)\n",
    "SeasonPercentage = reduce(lambda x,y: x+y,SeasonPercentage)\n",
    "y = {'team':team, 'season': season, 'SeasonPercentage': SeasonPercentage}\n",
    "y = pd.DataFrame(y)\n",
    "df = pd.merge(df, y, on=['team', 'season'], how='left')\n",
    "df = df.drop(['team', 'season', 'Lebron James:steals', 'Lebron James:fouls', 'Lebron James:blocks', 'Lebron James:free throws attempted'], axis=1)\n",
    "\n",
    "print(len(df))\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of multicollinearity\n",
    "df1 = df.drop(['W'], axis=1)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X = add_constant(df1)\n",
    "pd.Series([variance_inflation_factor(X.values, i)\n",
    "               for i in range(X.shape[1])],\n",
    "              index=X.columns)\n",
    "# no need for a feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Validation scores on 4 folds of train and test sets in order to prevent overfitting while controling the C parameter of Logistic Regression:\n",
    "    ### 4 splits ensure that all the data points will be tested\n",
    "        ### later, the 75:25 (train set : test set) will be used\n",
    "        \n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### Splitting the data on dependent and independet variables\n",
    "X = df1\n",
    "y = df['W']\n",
    "\n",
    "### Testing the validation scores on a range of C parameters in order to find a suitable \n",
    "param_range = np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "train_scores, test_scores = validation_curve(LogisticRegression(), X, y,\n",
    "                                            param_name='C',\n",
    "                                            param_range=param_range, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with Logistic Regression')\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 1\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Validation Curve, we can see that the data set doesn't tend to over-fit -> the complexity of the dataset is in order.\n",
    "# When the C parameter = 1, the Cross-Validation score has the best performance.\n",
    "# Let's further investigate C = [0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the results, let's further investigate the accuracy, AUC and recall.\n",
    "\n",
    "# Cross-validation with the test for the variance of C-parameter (narrowed down to tree options based on the Validation Curve)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for c_param in [0.1, 1, 10]:\n",
    "    clf = LogisticRegression(C=c_param)\n",
    "\n",
    "    # accuracy is the default scoring metric\n",
    "    print('For C parameter = {0}'.format(c_param) + ':')\n",
    "    print('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=4))\n",
    "    # use AUC as scoring metric\n",
    "    print('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=4, scoring = 'roc_auc'))\n",
    "    # use recall as scoring metric\n",
    "    print('Cross-validation (recall)', cross_val_score(clf, X, y, cv=4, scoring = 'recall'))\n",
    "       # use recall as scoring metric\n",
    "    print('Cross-validation (precesion)', cross_val_score(clf, X, y, cv=4, scoring = 'precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the performance with the C = 1 is performing better than C=10.\n",
    "# Also, the C=1 model outperforms the C=0.01 when it comes to AUC and accuracy. The trade off between racall and precision is relatively similar to the previous case (without the feature: Season Percentage)\n",
    "# # Based on the same arguments introduced in the 1st model, the C=1 is a suitable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the Confusion Matrix, Accuracy, Precision, and Recall for C=1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "clf_predicted = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, clf_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, clf_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the results from Model with/without the feature \"Season Percentage\" on the same train and test sets:\n",
    "    # the new feature improved the model by 1% of accuracy, 2% of precision. 2% of recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Precision-recall curves\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ### Decision functions\n",
    "y_scores_lr = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recall and Precision trade-off is captured above. The current state of the model displayed by the red circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves for C=0,1 and C = 1\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "for g in [0.1, 1]:\n",
    "    clf = LogisticRegression(C=g).fit(X_train, y_train)\n",
    "    y_score_svm = clf.decision_function(X_test)\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    accuracy_svm = clf.score(X_test, y_test)\n",
    "    print(\"C = {:.3f}  accuracy = {:.3f}   AUC = {:.3f}\".format(g, accuracy_svm,\n",
    "                                                                    roc_auc_svm))\n",
    "    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7,\n",
    "             label='(C = {:0.3f}, area = {:0.3f})'.format(g, roc_auc_svm))\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "plt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.title('ROC curve', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The AUC for C=1 is higher, which also supports the decision to build the model this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the choice of the C parameter was done.\n",
    "# The model complexity is not leading to over/under fitting => tested on 4 folds. \n",
    "# Let's fit the model to all the data points available in order to achieve the best performance\n",
    "  # -> the number of observation has nothing to do with over fitting -> that is problem of the model complexity\n",
    "  \n",
    "# Creating new observations from the NBA Finals Game 1,2,3,4 (previously not seen in the data set) -> and predict the W probability for these games.\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X, y)\n",
    "\n",
    "# G1:\n",
    "LBJ_stats_first_game_1 = {'Lebron James:points': [51],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:field goals attempted': [32],\n",
    "                        'Lebron James:minutes': [48],\n",
    "                        'Lebron James:three pointers attempted': [7],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:rebounds':[8],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g1 = pd.DataFrame(data=LBJ_stats_first_game_1)\n",
    "y_predict1 = clf.predict_proba(g1)\n",
    "\n",
    "#G2:\n",
    "LBJ_stats_first_game_2 = {'Lebron James:points': [29],\n",
    "                        'Lebron James:assists': [13],\n",
    "                        'Lebron James:field goals attempted': [20],\n",
    "                        'Lebron James:minutes': [44],\n",
    "                        'Lebron James:three pointers attempted': [4],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:rebounds':[9],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g2 = pd.DataFrame(data=LBJ_stats_first_game_2)\n",
    "y_predict2 = clf.predict_proba(g2)\n",
    "\n",
    "#G3:\n",
    "LBJ_stats_first_game_3 = {'Lebron James:points': [33],\n",
    "                        'Lebron James:assists': [11],\n",
    "                        'Lebron James:field goals attempted': [28],\n",
    "                        'Lebron James:minutes': [47],\n",
    "                        'Lebron James:three pointers attempted': [6],\n",
    "                        'Lebron James:turnovers': [4],\n",
    "                        'Lebron James:rebounds':[10],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g3 = pd.DataFrame(data=LBJ_stats_first_game_3)\n",
    "y_predict3 = clf.predict_proba(g3)\n",
    "\n",
    "#G4:\n",
    "LBJ_stats_first_game_4 = {'Lebron James:points': [23],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:field goals attempted': [13],\n",
    "                        'Lebron James:minutes': [41],\n",
    "                        'Lebron James:three pointers attempted': [1],\n",
    "                        'Lebron James:turnovers': [6],\n",
    "                        'Lebron James:rebounds':[7],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "g4 = pd.DataFrame(data=LBJ_stats_first_game_4)\n",
    "y_predict4 = clf.predict_proba(g4)\n",
    "\n",
    "print('Probability of Cavs winning the G1: ', y_predict1)\n",
    "print('Probability of Cavs winning the G2: ', y_predict2)\n",
    "print('Probability of Cavs winning the G3: ', y_predict3)\n",
    "print('Probability of Cavs winning the G4: ', y_predict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -> Even when delivering great performance in G1 ad G2, the probability was only 63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lebron James would need to put up these number to increase the change of Cleveland beating GSW to 90%:\n",
    "LBJ_stats_first_game_X = {'Lebron James:points': [50],\n",
    "                        'Lebron James:assists': [15],\n",
    "                        'Lebron James:field goals attempted': [30],\n",
    "                        'Lebron James:minutes': [42],\n",
    "                        'Lebron James:three pointers attempted': [5],\n",
    "                        'Lebron James:turnovers': [2],\n",
    "                        'Lebron James:rebounds':[10],\n",
    "                        'SeasonPercentage': [0.707]}\n",
    "\n",
    "gX = pd.DataFrame(data=LBJ_stats_first_game_X)\n",
    "y_predictX = clf.predict_proba(gX)\n",
    "\n",
    "print('Probability of Cavs winning the GX: ', y_predictX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how the feature importance changed compare to the previous model:\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
    "                            random_state = 0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, list(X_train))\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))\n",
    "print(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmed: The quality of the opponent is the most important feature in this model, and it overshadows LeBron's statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic('matplotlib notebook')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Load Data Set with Lebron's statistics\n",
    "df = pd.read_csv('lebron_gameW_pred.csv', sep=';')\n",
    "df['W'] = np.where(df['W'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the lenght of the datasets, features, and data types\n",
    "len(df)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking on multicollinearity \n",
    "df1 = df.drop(['W'], axis=1)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Before checking the multicollinearity, adding the constant into the model\n",
    "X = add_constant(df1)\n",
    "pd.Series([variance_inflation_factor(X.values, i)\n",
    "               for i in range(X.shape[1])],\n",
    "              index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping out conflicting features based on the previous results\n",
    "# One by one -> drop one feature and then check again as long as a multicollinearity is present\n",
    "df1 = df.drop(['W', 'Lebron James:three pointers made', 'Lebron James:defensive rebounds', 'Lebron James:offensive rebounds', 'Lebron James:field goals made', 'Lebron James:free throws made'], axis=1)\n",
    "\n",
    "# The final pick of features and the results of multicollinearity check\n",
    "X = add_constant(df1)\n",
    "pd.Series([variance_inflation_factor(X.values, i)\n",
    "               for i in range(X.shape[1])],\n",
    "              index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Validation scores on 4 folds of train and test sets in order to prevent overfitting while controlling the C parameter of Logistic Regression:\n",
    "    ### 4 splits ensure that all the data points will be tested\n",
    "        ### later, the 75:25 (train set : test set) will be used\n",
    "        \n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### Splitting the data on dependent and independet variables\n",
    "X = df1\n",
    "y = df['W']\n",
    "\n",
    "### Testing the validation scores on a range of C parameters in order to find a suitable \n",
    "param_range = np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "train_scores, test_scores = validation_curve(LogisticRegression(), X, y,\n",
    "                                            param_name='C',\n",
    "                                            param_range=param_range, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "# See:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with Logistic Regression')\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 1\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Validation Curve, we can see that the data set doesn't tend to significantly over-fit in any stage of the validation.\n",
    "# When the C parameter = 1, the Cross-Validation score has the lowest spread. \n",
    "# But we should also pay attention to the results when the C parameter = 0.001, and investigate the performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the results, let's further investigate the accuracy, AUC, recall and precision.\n",
    "\n",
    "##### Cross-validation with the test for the variance of C-parameter (narrowed down to tree options based on the Validation Curve.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for c_param in [0.001, 1, 10]:\n",
    "    clf = LogisticRegression(C=c_param)\n",
    "\n",
    "    # accuracy is the default scoring metric\n",
    "    print('For C parameter = {0}'.format(c_param) + ':')\n",
    "    print('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=4))\n",
    "    # use AUC as scoring metric\n",
    "    print('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=4, scoring = 'roc_auc'))\n",
    "    # use recall as scoring metric\n",
    "    print('Cross-validation (recall)', cross_val_score(clf, X, y, cv=4, scoring = 'recall'))\n",
    "       # use recall as scoring metric\n",
    "    print('Cross-validation (precesion)', cross_val_score(clf, X, y, cv=4, scoring = 'precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the performance with the C = 10 is performing worse than C=1, \n",
    "    # -> the first fold suggest that the accuracy and also recall is bellow 0.7, while the rest of the fold is nearly identical.\n",
    "\n",
    "# Comparing C = 0.001 and C=1:\n",
    "#  -> the accuracy is played by the C=1 (2 folds are better, 1 equal and 1 worse)\n",
    "#  -> Area Under the Curve is also better for C=1\n",
    "#  -> The recall is much better in all folds for C = 0.001\n",
    "    # however, that come with the price of lower precision. Yes, we aim for high recall, but... \n",
    "    # as the precission would often decrease under the level of 75%, we could be dealing with the\n",
    "    # fact the model becomes a dummy classifier and detects (almost) everything as a W\n",
    "    # this is not the desired outcome of the model, and therefore C=1 is a better option\n",
    "\n",
    "# The goals it to create a classifier that will compute the probability of a Win (positive class) based on Lebron James' statistics from the game,\n",
    "    # therefore I'd like to tune the model the way that the threshold for the W class is low (higher recall than precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the Confusion Matrix, Accuracy, Precision, and Recall for C=1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "clf_predicted = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, clf_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, clf_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Precision-recall curves\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ### Decision functions\n",
    "y_scores_lr = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recall and Precision trade-off is captured above. The current state of the model displayed by the red circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves for C=0,001 and C = 1\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "for g in [0.001, 1]:\n",
    "    clf = LogisticRegression(C=g).fit(X_train, y_train)\n",
    "    y_score_svm = clf.decision_function(X_test)\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    accuracy_svm = clf.score(X_test, y_test)\n",
    "    print(\"C = {:.3f}  accuracy = {:.3f}   AUC = {:.3f}\".format(g, accuracy_svm,\n",
    "                                                                    roc_auc_svm))\n",
    "    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7,\n",
    "             label='(C = {:0.3f}, area = {:0.3f})'.format(g, roc_auc_svm))\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "plt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.title('ROC curve', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The AUC for C=1 is higher, which also supports the decision to build the model this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the choice of the C parameter was done.\n",
    "# The model complexity is not leading to over/under fitting => tested on 4 folds. \n",
    "# Let's fit the model to all the data points available in order to achieve the best performance\n",
    "  # -> the number of observation has nothing to do with over fitting -> that is problem of the model complexity\n",
    "\n",
    "clf = LogisticRegression(C=1).fit(X, y)\n",
    "\n",
    "# Creating new observations from the NBA Finals Game 1,2,3,4 (previously not seen in the data set) -> and predict the W probability for these games.\n",
    "# G1:\n",
    "LBJ_stats_first_game_1 = {'Lebron James:points': [51],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:blocks':[1],\n",
    "                        'Lebron James:field goals attempted': [32],\n",
    "                        'Lebron James:minutes': [48],\n",
    "                        'Lebron James:fouls':[2],\n",
    "                        'Lebron James:steals': [1],\n",
    "                        'Lebron James:three pointers attempted': [7],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:free throws attempted':[11],\n",
    "                        'Lebron James:rebounds':[8]}\n",
    "\n",
    "g1 = pd.DataFrame(data=LBJ_stats_first_game_1)\n",
    "y_predict1 = clf.predict_proba(g1)\n",
    "#G2:\n",
    "LBJ_stats_first_game_2 = {'Lebron James:points': [29],\n",
    "                        'Lebron James:assists': [13],\n",
    "                        'Lebron James:field goals attempted': [20],\n",
    "                        'Lebron James:blocks':[0],\n",
    "                        'Lebron James:minutes': [44],\n",
    "                        'Lebron James:fouls':[2],\n",
    "                        'Lebron James:steals': [2],\n",
    "                        'Lebron James:three pointers attempted': [4],\n",
    "                        'Lebron James:turnovers': [5],\n",
    "                        'Lebron James:free throws attempted':[9],\n",
    "                        'Lebron James:rebounds':[9]}\n",
    "\n",
    "g2 = pd.DataFrame(data=LBJ_stats_first_game_2)\n",
    "y_predict2 = clf.predict_proba(g2)\n",
    "#G3:\n",
    "LBJ_stats_first_game_3 = {'Lebron James:points': [33],\n",
    "                        'Lebron James:assists': [11],\n",
    "                        'Lebron James:blocks':[2],                          \n",
    "                        'Lebron James:field goals attempted': [28],\n",
    "                        'Lebron James:minutes': [47],\n",
    "                        'Lebron James:fouls':[2],\n",
    "                        'Lebron James:steals': [2],\n",
    "                        'Lebron James:three pointers attempted': [6],\n",
    "                        'Lebron James:turnovers': [4],\n",
    "                        'Lebron James:free throws attempted':[7],\n",
    "                        'Lebron James:rebounds':[10]}\n",
    "\n",
    "g3 = pd.DataFrame(data=LBJ_stats_first_game_3)\n",
    "y_predict3 = clf.predict_proba(g3)\n",
    "\n",
    "#G4:\n",
    "LBJ_stats_first_game_4 = {'Lebron James:points': [23],\n",
    "                        'Lebron James:assists': [8],\n",
    "                        'Lebron James:field goals attempted': [13],                          \n",
    "                        'Lebron James:blocks':[1],\n",
    "                        'Lebron James:minutes': [41],\n",
    "                        'Lebron James:fouls':[5],\n",
    "                        'Lebron James:steals': [0],\n",
    "                        'Lebron James:three pointers attempted': [1],\n",
    "                        'Lebron James:turnovers': [6],\n",
    "                        'Lebron James:free throws attempted':[11],\n",
    "                        'Lebron James:rebounds':[7]}\n",
    "\n",
    "g4 = pd.DataFrame(data=LBJ_stats_first_game_4)\n",
    "y_predict4 = clf.predict_proba(g4)\n",
    "\n",
    "print('Probability of Cavs winning the G1: ', y_predict1)\n",
    "print('Probability of Cavs winning the G2: ', y_predict2)\n",
    "print('Probability of Cavs winning the G3: ', y_predict3)\n",
    "print('Probability of Cavs winning the G4: ', y_predict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, the model was 4 times wrong -> as we know, Cavaliers lost all the 4 games.\n",
    "# What does this mean? Basically just two things. And perhaps, most of us knew this all along:\n",
    "    # 1.: Lebron James had done enough to increase the chance of the W in all 4 games, and it wasn't his fault that Cleveland had lost.\n",
    "    # 2.: The model is not good enough to get close to the real probability. It simply needs more data to learn. \n",
    "\n",
    "# So let's feed it with one more feature -> the quality of the opponents Team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model regularization, we might use one of many approaches... starting with Lasso, Ridge regression... ending with Recursive Feature Elimination with Scikit Learn.\n",
    "# Also, we can create a decision tree and see the feature importance and eliminate few features this way.\n",
    "# #### Creating a Decision Tree Classifier\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
    "                            random_state = 0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance and eliminating features from the next model: blocks, fouls, free throws attempted, steals.\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, list(X_train))\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))\n",
    "print(list(X_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
